expanded_var = formula(medv~poly(lstat,degree = 3,raw = TRUE)
+poly(crim,degree = 3,raw = TRUE)+(zn > 0)+poly(zn,degree = 3,raw = TRUE)
+poly(indus,degree = 3,raw = TRUE)+chas+poly(nox,degree = 3,raw = TRUE)
+poly(rm,degree = 3,raw = TRUE)+poly(age,degree = 3,raw = TRUE)
+poly(dis,degree = 3,raw = TRUE)+poly(rad,degree = 3,raw = TRUE)
+poly(tax,degree = 3,raw = TRUE)+poly(ptratio,degree = 3,raw = TRUE)
+poly(black,degree = 3,raw = TRUE))
#perform best subset selection:
best_subset = regsubsets(expanded_var, data = train_set, nvmax = 39, method = "exhaustive")
#Summary of the results
summary_best_subset = summary(best_subset)
#print results
summary_best_subset
#Select AIC and BIC using variance estimate from the model (take Cp/BIC computed by regsubsets())
kbic_reg=which.min(summary_best_subset$bic) #BIC choice = 16
kbic_reg
kaic_reg=which.min(summary_best_subset$cp)  #AIC choice (AIC is proportional to Cp and so ranking is the same) = 29
kaic_reg
#for kicks, see the model with the highest adjusted R-squared - does it overfit?
which.max(summary_best_subset$adjr2)
#output = 29
#Plot the standardized IC curves (we subtract the mean and divide by variance):
plot((summary_best_subset$bic-mean(summary_best_subset$bic))/sd(summary_best_subset$bic), main = "AIC and BIC (Standardized generated by regsubsets())", xlab = "k",
ylab = "IC", type = "l", col = "blue")
points((summary_best_subset$cp-mean(summary_best_subset$cp))/sd(summary_best_subset$cp), type = "l", col = "red")
legend("topright", c("BIC","AIC"),lty=c(1,1) ,col=c("blue","red"))
#Select AIC and BIC using error variance estimate from the largest model
ntrain = 400
variance_estimate=summary_best_subset$rss[38]/(ntrain-39) #estimate error variance of the model with k=38
#We divide by ntrain-39 because there are 38 regressors+constant=39 parameters
#construct the IC with this estimate (note seq() used to generate a sequence from 1 to 38 regressors to plug into the formula):
BICL = summary_best_subset$rss/ntrain + log(ntrain)*variance_estimate*((seq(1,38,1))/ntrain)
AICL = summary_best_subset$rss/ntrain + 2*variance_estimate*((seq(1,38,1))/ntrain)
kbic_varlargest=which.min(BICL) #BIC choice = 16
kbic_varlargest
kaic_varlargest=which.min(AICL)  #AIC choice (AIC proportional to Cp and so ranking is the same) = 29
kaic_varlargest
#######################################
#AIC and BIC using iterative procedure
#######################################
sig0=var(train_set$medv) #sample variance of Y
#Compute IC for the models using sigma0 estimate:
BIC0 = summary_best_subset$rss/ntrain + log(ntrain)*sig0*((seq(1,38,1))/ntrain)
AIC0 = summary_best_subset$rss/ntrain + 2*sig0*((seq(1,38,1))/ntrain)
#Select best models
k0bic = which.min(BIC0)
k0bic
k0aic = which.min(AIC0)
k0aic
#Obtain IC estimates with sigma 1 estimate:
BIC1 = summary_best_subset$rss/ntrain + log(ntrain)*(summary_best_subset$rss[k0bic]/(ntrain-k0bic-1))*((seq(1,38,1))/ntrain)
AIC1 = summary_best_subset$rss/ntrain + 2*(summary_best_subset$rss[k0aic]/(ntrain-k0aic-1))*((seq(1,38,1))/ntrain)
k1bic = which.min(BIC1)
k1bic
k1aic = which.min(AIC1)
k1aic
#Iterate once more to check:
BIC2 = summary_best_subset$rss/ntrain + log(ntrain)*(summary_best_subset$rss[k1bic]/(ntrain-k1bic-1))*((seq(1,38,1))/ntrain)
AIC2 = summary_best_subset$rss/ntrain + 2*(summary_best_subset$rss[k1aic]/(ntrain-k1aic-1))*((seq(1,38,1))/ntrain)
k2bic = which.min(BIC2)
k2bic
#bic = 13
k2aic = which.min(AIC2)
k2aic
#aic = 25
BIC3 = summary_best_subset$rss/ntrain + log(ntrain)*(summary_best_subset$rss[k2bic]/(ntrain-k2bic-1))*((seq(1,38,1))/ntrain)
AIC3 = summary_best_subset$rss/ntrain + 2*(summary_best_subset$rss[k2aic]/(ntrain-k2aic-1))*((seq(1,38,1))/ntrain)
k3bic = which.min(BIC3)
k3bic
#bic = 16
k3aic = which.min(AIC3)
k3aic
#aic = 29
#See here we can stop after three steps ( the choice converges thereafter)
## Calculate OOS MSE using AIC and BIC Minimizing models:
#Get the X-matrix for the test set:
test.mat = model.matrix(expanded_var, data = test_set)
#extract coefficients from the best model on BIC
temp.coef = coef(best_subset, id = kbic_reg)
mean((test_set$medv-test.mat[,names(temp.coef)]%*%temp.coef)^2)
MSEBIC = mean((test_set$medv-test.mat[,names(temp.coef)]%*%temp.coef)^2)
MSEBIC
varselbic = names(temp.coef)  # Selected variables on BIC
varselbic
temp.coef = coef(best_subset, id = kaic_reg)
MSEAIC = mean((test_set$medv-test.mat[,names(temp.coef)]%*%temp.coef)^2)
MSEAIC
varselaic = names(temp.coef)  # Selected variables on AIC
varselaic
temp.coef = coef(best_subset, id = kbic_varlargest)
MSEBICL = mean((test_set$medv-test.mat[,names(temp.coef)]%*%temp.coef)^2)
MSEBICL
temp.coef = coef(best_subset, id = kaic_varlargest)
MSEAICL = mean((test_set$medv-test.mat[,names(temp.coef)]%*%temp.coef)^2)
MSEAICL
temp.coef = coef(best_subset, id = k3bic)
MSEBIC1 = mean((test_set$medv-test.mat[,names(temp.coef)]%*%temp.coef)^2)
MSEBIC1
temp.coef = coef(best_subset, id = k3aic)
MSEAIC1 = mean((test_set$medv-test.mat[,names(temp.coef)]%*%temp.coef)^2)
MSEAIC1
#########################################################
#Part 3
#########################################################
rm(list = ls())
df=Boston
attach(Boston)
#Set the training set size:
ntrain=400
set.seed(1789424) #seed of the random number generator for replicability
tr = sample(1:nrow(df),ntrain)  # randomly sampled ntrain indices of observations
train = df[tr,]   # get the training sample
test = df[-tr,]   # get the testing sample
#Here is some feature engineering: obtain big data from small data!
## Get large set of variables through transformations:
#First, we will form dummy variables based on deciles of continuous variables.
#Step 1 - form deciles using the familiar quantile() function (deciles coded by sequence from 0 to 1 in increments of 0.1)
#Step 2 - replace first and last value by a very low/high number, respectively
#Step 3 - create a categorical variable with a category defined by belonging to a particular decile.
#for continuous variables:
zz = unique(quantile(train$lstat, probs = seq(0,1,.1))) #NB: unique() here and below is used here just in case to remove duplicate values which may sometimes occur in computing empirical quantiles
zz[1] = -1e30 #to capture anything lower than estimated lower bound for the first decile
zz[length(zz)] = 1e30 #to capture anything higher than the estimated higher bound for the last decile
lstatInt = (cut(lstat,zz,include.lowest = TRUE)) #create categorical variable defined by decile intervals - similar to 10-fold cross-validation earlier trick
#Do this for the rest of continuous variables
zz = unique(quantile(train$crim, probs = seq(0,1,.1)))
zz[1] = -1e30
zz[length(zz)] = 1e30
crimInt = (cut(crim,zz,include.lowest = TRUE))
zz = unique(quantile(train$zn, probs = seq(0,1,.1)))
zz[1] = -1e30
zz[length(zz)] = 1e30
znInt = (cut(zn,zz,include.lowest = TRUE))
zz = unique(quantile(train$indus, probs = seq(0,1,.1)))
zz[1] = -1e30
zz[length(zz)] = 1e30
indusInt = (cut(indus,zz,include.lowest = TRUE))
zz = unique(quantile(train$nox, probs = seq(0,1,.1)))
zz[1] = -1e30
zz[length(zz)] = 1e30
noxInt = (cut(nox,zz,include.lowest = TRUE))
zz = unique(quantile(train$rm, probs = seq(0,1,.1)))
zz[1] = -1e30
zz[length(zz)] = 1e30
rmInt = (cut(rm,zz,include.lowest = TRUE))
zz = unique(quantile(train$age, probs = seq(0,1,.1)))
zz[1] = -1e30
zz[length(zz)] = 1e30
ageInt = (cut(age,zz,include.lowest = TRUE))
zz = unique(quantile(train$dis, probs = seq(0,1,.1)))
zz[1] = -1e30
zz[length(zz)] = 1e30
disInt = (cut(dis,zz,include.lowest = TRUE))
zz = unique(quantile(train$rad, probs = seq(0,1,.1)))
zz[1] = -1e30
zz[length(zz)] = 1e30
radInt = (cut(rad,zz,include.lowest = TRUE))
zz = unique(quantile(train$tax, probs = seq(0,1,.1)))
zz[1] = -1e30
zz[length(zz)] = 1e30
taxInt = (cut(tax,zz,include.lowest = TRUE))
zz = unique(quantile(train$ptratio, probs = seq(0,1,.1)))
zz[1] = -1e30
zz[length(zz)] = 1e30
ptratioInt = (cut(ptratio,zz,include.lowest = TRUE))
zz = unique(quantile(train$black, probs = seq(0,1,.1)))
zz[1] = -1e30
zz[length(zz)] = 1e30
blackInt = (cut(black,zz,include.lowest = TRUE))
#Append all the new categorical variables to the current dataframe:
df = cbind(df,lstatInt,crimInt,znInt,indusInt,noxInt,rmInt,ageInt,disInt,radInt,taxInt,ptratioInt,blackInt)
#Redefine training and test samples
train = df[tr,]   # Training sample
test = df[-tr,]   # Testing sample
#Use formula to create polynomials and interactions:
#Here we create:
#Cubic polynomials in all continuous variables together with first and second order interactions
#Interactions of lstat and dis decile dummies with 6th order polynomials ins lstat and dis and cubics in crim,nox,tax,ptratio
#Interactions of other decile dummies with cubics in their base variables
fbfs = formula(medv~chas+(poly(cbind(lstat,crim,zn,indus,nox,rm,age,dis,rad,tax,ptratio,black),degree=3,raw=TRUE)
+lstatInt*(poly(lstat,degree = 6,raw = TRUE)+poly(crim,degree = 3,raw = TRUE)
+poly(nox,degree = 3,raw = TRUE)+poly(tax,degree = 3,raw = TRUE)
+poly(dis,degree = 6,raw = TRUE)+poly(ptratio,degree = 3,raw = TRUE))
+crimInt*poly(crim,degree = 3,raw = TRUE)+znInt*poly(zn,degree = 3,raw = TRUE)
+indusInt*poly(indus,degree = 3,raw = TRUE)+noxInt*poly(nox,degree = 3,raw = TRUE)
+rmInt*poly(rm,degree = 3,raw = TRUE)+ageInt*poly(age,degree = 3,raw = TRUE)
+disInt*(poly(lstat,degree = 6,raw = TRUE)+poly(crim,degree = 3,raw = TRUE)
+poly(nox,degree = 3,raw = TRUE)+poly(tax,degree = 3,raw = TRUE)
+poly(dis,degree = 6,raw = TRUE)+poly(ptratio,degree = 3,raw = TRUE))
+radInt*poly(rad,degree = 3,raw = TRUE)
+taxInt*poly(tax,degree = 3,raw = TRUE)+ptratioInt*poly(ptratio,degree = 3,raw = TRUE)
+blackInt*poly(black,degree = 3,raw = TRUE)))
# Prepare data for use with glmnet
testfs.mat = model.matrix(fbfs, data = test)
trainfs.mat = model.matrix(fbfs, data = train)
#Some cleanup,
#First, find out which columns (i.e. variables) contain NaN values after standardization
#This is helpful to weed out predictor that do not vary (i.e., are constant, since variance is zero and division by zero gives NaN)
#In our case, we will pick up Intercept, which is automatically included by model.matrix()
drop1 = which(is.nan(colMeans(scale(trainfs.mat)))) #NB: scale standardizes (-mean, divide by variance); colMeans takes mean by column. If a column contains a NaN, the mean is also a NaN
#Because demeaning a constant yields zero, and standard deviation is 0 - the resulting standardization is 0/0=NaN
#We also need to watch out for duplicate regressors, which sometimes occur when we mass-produce polynomial/interaction terms etc.
drop2 = which(duplicated(trainfs.mat, MARGIN = 2)) #duplicated(X,MARGIN=2) searches for duplicate columns of matrix X. Aside: (MARGIN=1 would search for duplicate rows)
#Join the problematic variable indices:
drop = union(drop1,drop2)
#Remove problematic variables from both test and train sets:
testfs.mat = testfs.mat[,-drop]
trainfs.mat = trainfs.mat[,-drop]
#num of variables = 1203
#####################################################################################
## Part 4
## Forward stepwise selection (the code here is essentially the same as above, with
## the only difference being the "forward" option in the regsubsets() function)
## maximum model size at 200 variables
#####################################################################################
forward_subset = regsubsets(x=trainfs.mat, y=as.matrix(train['medv']), nvmax = 200, method = "forward")
summary_forward_subset = summary(forward_subset)
#print results
summary_forward_subset
#######################################
#AIC and BIC using iterative procedure
#######################################
sig0=var(train$medv) #sample variance of Y
#Compute IC for the models using sigma0 estimate:
BIC0 = summary_forward_subset$rss/ntrain + log(ntrain)*sig0*((seq(1,201,1))/ntrain)
AIC0 = summary_forward_subset$rss/ntrain + 2*sig0*((seq(1,201,1))/ntrain)
#Select best models
k0bic = which.min(BIC0)
k0bic
k0aic = which.min(AIC0)
k0aic
#Obtain IC estimates with sigma 1 estimate:
BIC1 = summary_forward_subset$rss/ntrain + log(ntrain)*(summary_forward_subset$rss[k0bic]/(ntrain-k0bic-1))*((seq(1,201,1))/ntrain)
AIC1 = summary_forward_subset$rss/ntrain + 2*(summary_forward_subset$rss[k0aic]/(ntrain-k0aic-1))*((seq(1,201,1))/ntrain)
k1bic = which.min(BIC1)
k1bic
k1aic = which.min(AIC1)
k1aic
#Iterate once more to check:
BIC2 = summary_forward_subset$rss/ntrain + log(ntrain)*(summary_forward_subset$rss[k1bic]/(ntrain-k1bic-1))*((seq(1,201,1))/ntrain)
AIC2 = summary_forward_subset$rss/ntrain + 2*(summary_forward_subset$rss[k1aic]/(ntrain-k1aic-1))*((seq(1,201,1))/ntrain)
k2bic = which.min(BIC2)
k2bic
k2aic = which.min(AIC2)
k2aic
BIC3 = summary_forward_subset$rss/ntrain + log(ntrain)*(summary_forward_subset$rss[k2bic]/(ntrain-k2bic-1))*((seq(1,201,1))/ntrain)
AIC3 = summary_forward_subset$rss/ntrain + 2*(summary_forward_subset$rss[k2aic]/(ntrain-k2aic-1))*((seq(1,201,1))/ntrain)
k3bic = which.min(BIC3)
k3bic
k3aic = which.min(AIC3)
k3aic
BIC4 = summary_forward_subset$rss/ntrain + log(ntrain)*(summary_forward_subset$rss[k3bic]/(ntrain-k3bic-1))*((seq(1,201,1))/ntrain)
AIC4 = summary_forward_subset$rss/ntrain + 2*(summary_forward_subset$rss[k3aic]/(ntrain-k3aic-1))*((seq(1,201,1))/ntrain)
k4bic = which.min(BIC4)
k4bic
k4aic = which.min(AIC4)
k4aic
BIC5 = summary_forward_subset$rss/ntrain + log(ntrain)*(summary_forward_subset$rss[k4bic]/(ntrain-k4bic-1))*((seq(1,201,1))/ntrain)
AIC5 = summary_forward_subset$rss/ntrain + 2*(summary_forward_subset$rss[k4aic]/(ntrain-k4aic-1))*((seq(1,201,1))/ntrain)
k5bic = which.min(BIC5)
k5bic
k5aic = which.min(AIC5)
k5aic
BIC6 = summary_forward_subset$rss/ntrain + log(ntrain)*(summary_forward_subset$rss[k5bic]/(ntrain-k5bic-1))*((seq(1,201,1))/ntrain)
AIC6 = summary_forward_subset$rss/ntrain + 2*(summary_forward_subset$rss[k5aic]/(ntrain-k5aic-1))*((seq(1,201,1))/ntrain)
k6bic = which.min(BIC6)
k6bic
k6aic = which.min(AIC6)
k6aic
BIC7 = summary_forward_subset$rss/ntrain + log(ntrain)*(summary_forward_subset$rss[k6bic]/(ntrain-k6bic-1))*((seq(1,201,1))/ntrain)
AIC7 = summary_forward_subset$rss/ntrain + 2*(summary_forward_subset$rss[k6aic]/(ntrain-k6aic-1))*((seq(1,201,1))/ntrain)
k7bic = which.min(BIC7)
k7bic
k7aic = which.min(AIC7)
k7aic
#bic = 28
#aic = 188
#extract coefficients from the best model on BIC
temp.coef = coef(forward_subset, id = k7bic)
test_new.mat = model.matrix(fbfs, data = test)
MSEBIC = mean((test$medv-test_new.mat[,names(temp.coef)]%*%temp.coef)^2)
MSEBIC
temp.coef = coef(forward_subset, id = k7aic)
MSEAICL = mean((test$medv-test_new.mat[,names(temp.coef)]%*%temp.coef)^2)
MSEAICL
#####################################################################################
## Part 5
#####################################################################################
set.seed(1789424)
#function(input1, input2,...) defines an R function; note the input order matters when calling;
MSE <- function(pred, truth){ #start and end body of the function by { } - same as a loop
return(mean((truth - pred)^2)) #end function with a return(output) statement. Here we can go straight to return because the object of interest is a simple function of inputs
}
#already does not contain intercept so it's fine for Ridge regression which runs on demeaned data without one
x = trainfs.mat
y = train['medv'] #define Y
#For ridge regression, we need to choose a value for lambda. Although glmnet() defaults to an automatically selected range, we can also supply our own values as we'll do here.
grid = 10^seq(10, -2, length = 100)#an example of a grid that starts denser and becomes sparser as lambda grows - taken from ISLR lab in chapter 6
#you can of course define any grid that makes sense in your applications. Several grids can be tried zooming in on "promising" intervals.
#Example of fitting the ridge model
#The argument 0 <= alpha <= 1 is the elastic net mixing parameter. A value of 0 yields ridge and a value of 1 yields Lasso (1 is default). 0 < alpha <1 yields the Elastic net, which is a combination of ridge and LASSO penalties.
ridge.mod = glmnet(x, data.matrix(y), alpha = 0, lambda = grid)
#Now try 10-fold CV with a user-defined grid:
cv.out10u = cv.glmnet(x, data.matrix(y), alpha = 0, lambda=grid)
cv.out10u
#Lambda Measure     SE Nonzero
#min  932.6   52.37 11.891    1203
#1se 2848.0   61.19  8.812    1203
plot(cv.out10u) #automatic plot
plot(cv.out10u$lambda,cv.out10u$cvm, main="Ridge 10-fold CV user-defined grid", xlab="Lambda", ylab="CV MSE") #plot shown in lecture - I find this one more intuitive
#What is the RMSE associated with the value of lambda chosen by cross-validation?
x_test = testfs.mat
ridge.pred <- predict(ridge.mod, s = cv.out10u$lambda.1se, newx = x_test)
MSE(ridge.pred, data.matrix(test['medv']))
#value = 58.32706
#####################################################################################
## Part 6
#####################################################################################
#Run the LASSO. The only change is that we need to set alpha = 1 rather than 0 (alpha=1 is also the default setting).
lasso.mod <- glmnet(x, data.matrix(y), alpha = 1, lambda = grid)
plot(lasso.mod) #this allows you to plot coefficients as a function of your L1-budget (and hence lambda)
#This is may be useful to see how and where shrinkage kicks in.
set.seed(1789424) #reset seed so that the only difference in results is due to the grid choice
cv.out10l = cv.glmnet(x, data.matrix(y), alpha = 1, lambda=grid)
plot(cv.out10l)
plot(cv.out10l$lambda,cv.out10l$cvm, main="10-fold CV user-defined grid", xlab="Lambda", ylab="CV MSE")
cv.out10l
#Measure: Mean-Squared Error
#Lambda Measure    SE Nonzero
#min 0.2848   12.62 3.050      40
#1se 0.6579   15.31 3.343      20
#What is the MSE associated with the value of lambda chosen by cross-validation?
#for lambda.min
lasso.pred <- predict(lasso.mod, s= cv.out10l$lambda.min, newx = x_test)
MSE(lasso.pred, data.matrix(test['medv']))
#ans: 18.92397
#for lambda.lse
lasso.pred2 <- predict(lasso.mod, s = cv.out10l$lambda.1se, newx = x_test)
MSE(lasso.pred2, data.matrix(test['medv']))
set.seed(1789424)
#CASE 1: homoscedastic and iid data
rlasso(x=x, y= as.matrix(y['medv']), post=FALSE, homoscedastic = TRUE)
rlasso(unlist(y)~x,  post=TRUE, penalty = list(homoscedastic = TRUE))
rlasso.fit = rlasso(unlist(y)~x,  post=FALSE, homoscedastic = TRUE)
yhat.rlasso<- predict(rlasso.fit, newdata=x_test)
MSE(yhat.rlasso, data.matrix(test['medv']))
rlasso.fit = rlasso(unlist(y)~x,  post=TRUE, penalty = list(homoscedastic = TRUE))
yhat.rlasso<- predict(rlasso.fit, newdata=x_test)
MSE(yhat.rlasso, data.matrix(test['medv']))
rlasso.fit = rlasso(unlist(y)~x,  post=FALSE,  X.dependent.lambda = TRUE)
rlasso.fit = rlasso(unlist(y)~x,  post=FALSE)
yhat.rlasso<- predict(rlasso.fit, newdata=x_test)
MSE(yhat.rlasso, data.matrix(test['medv']))
rlasso.fit = rlasso(unlist(y)~x,  post=TRUE)
yhat.rlasso<- predict(rlasso.fit, newdata=x_test)
MSE(yhat.rlasso, data.matrix(test['medv']))
rlasso.fit = rlasso(unlist(y)~x,  post=TRUE, penalty = list(homoscedastic = TRUE))
yhat.rlasso<- predict(rlasso.fit, newdata=x_test)
MSE(yhat.rlasso, data.matrix(test['medv']))
rlasso.fit = rlasso(unlist(y)~x,  post=FALSE, penalty = list(homoscedastic = TRUE, X.dependent.lambda = FALSE))
yhat.rlasso<- predict(rlasso.fit, newdata=x_test)
MSE(yhat.rlasso, data.matrix(test['medv']))
rlasso.fit = rlasso(unlist(y)~x,  post=FALSE, penalty = list(homoscedastic = TRUE, X.dependent.lambda = TRUE))
yhat.rlasso<- predict(rlasso.fit, newdata=x_test)
MSE(yhat.rlasso, data.matrix(test['medv']))
rlasso.fit = rlasso(unlist(y)~x,  post=TRUE, penalty = list(homoscedastic = TRUE, X.dependent.lambda = TRUE))
yhat.rlasso<- predict(rlasso.fit, newdata=x_test)
MSE(yhat.rlasso, data.matrix(test['medv']))
rlasso.fit = rlasso(unlist(y)~x,  post=FALSE,  X.dependent.lambda = TRUE)
yhat.rlasso<- predict(rlasso.fit, newdata=x_test)
MSE(yhat.rlasso, data.matrix(test['medv']))
rlasso.fit = rlasso(unlist(y)~x,  post=TRUE, X.dependent.lambda = TRUE)
yhat.rlasso<- predict(rlasso.fit, newdata=x_test)
MSE(yhat.rlasso, data.matrix(test['medv']))
rm(list=ls())
library(forecast)
library(fpp)
library(ISLR)
library(glmnet)
library(hdm)
library("readxl")
library(psych) #install.packages("psych")
#read the file
df <- read_excel("Data.xlsx", sheet = "Data")
#assign x and y of the model
df$Date <- as.Date(df$Date, format = "%Y-%m-%d")
x = model.matrix(SP500EP ~ ., data = df)
x = x[,3:16] #remove constant and date
y = df$SP500EP
#assign the variables for test sets, training sets and validation sets
ntrain = 420 #data between 1980 to 2014
ntest = 60 #data between 2015 to 2019
xtrain = x[1 : ntrain,]
xtest = x[(ntrain + 1) : (ntrain + ntest),]
ytrain = y[1 : ntrain]
ytest = y[(ntrain + 1) : (ntrain + ntest)]
#read the file
df <- read_excel("/Users/Kaijing/documents/ec4308/Empirical-Performance-of-Equity-Premium-Prediction-on-Various-Machine-Learning/Data.xlsx", sheet = "Data")
#assign x and y of the model
df$Date <- as.Date(df$Date, format = "%Y-%m-%d")
x = model.matrix(SP500EP ~ ., data = df)
x = x[,3:16] #remove constant and date
y = df$SP500EP
#assign the variables for test sets, training sets and validation sets
ntrain = 420 #data between 1980 to 2014
ntest = 60 #data between 2015 to 2019
xtrain = x[1 : ntrain,]
xtest = x[(ntrain + 1) : (ntrain + ntest),]
ytrain = y[1 : ntrain]
ytest = y[(ntrain + 1) : (ntrain + ntest)]
View(df)
x = model.matrix(SP500EP ~ .-DATE, data = df)
x = model.matrix(SP500EP ~ .-Date, data = df)
View(x)
x = x[,2:15] #omit the intercept - Ridge regression is run on demeaned data without one
y = df$SP500EP #define Y
set.seed(12345)
train = df[xtrain,]
test = df[xtest,]
x_scaled <- scale(x)
aic <- c()
bic <- c()
lambdas_to_try <- 10^seq(-3, 5, length = 100)
#for all of the lambdas, run the model and compute AIC and BIC
for (lambda in seq(lambdas_to_try)) {
# Run model
model <- glmnet(x, y, alpha = 0, lambda = lambdas_to_try[lambda], standardize = TRUE)
# Extract coefficients and residuals (remove first row for the intercept)
betas <- as.vector((as.matrix(coef(model))[-1, ]))
resid <- y - (x_scaled %*% betas)
# Compute hat-matrix and degrees of freedom
ld <- lambdas_to_try[lambda] * diag(ncol(x_scaled))
H <- x_scaled %*% solve(t(x_scaled) %*% x_scaled + ld) %*% t(x_scaled)
#degrees of freedom is just the trace of the hat matrix
degfreedom <- tr(H)
# Compute information criteria
# assuming AIC(ridge) = nlog(e'e) + 2degfreedom(ridge)
#BIC(ridge) = nlog(e'e) + 2df(ridge)*log(n)
aic[lambda] <- nrow(x_scaled) * log(t(resid) %*% resid) + 2 * degfreedom
bic[lambda] <- nrow(x_scaled) * log(t(resid) %*% resid) + 2 * degfreedom * log(nrow(x_scaled))
}
# Plot information criteria against tried values of lambdas
plot(log(lambdas_to_try), aic, col = "orange", type = "l",
ylim = c(4000, 10000), ylab = "Information Criterion")
lines(log(lambdas_to_try), bic, col = "skyblue3")
legend("bottomright", lwd = 1, col = c("orange", "skyblue3"), legend = c("AIC", "BIC"))
# Optimal lambdas according to both criteria
lambda_aic <- lambdas_to_try[which.min(aic)]
lambda_aic
lambda_bic <- lambdas_to_try[which.min(bic)]
lambda_bic
# Fit final models, get their sum of squared residuals and mean squared error
model_aic <- glmnet(x, y, alpha = 0, lambda = lambda_aic, standardize = TRUE)
y_hat_aic <- predict(model_aic, x)
ssr_aic <- t(y - y_hat_aic) %*% (y - y_hat_aic)
mse_aic = mean((y - y_hat_aic)^2)
mse_aic
model_bic <- glmnet(x, y, alpha = 0, lambda = lambda_bic, standardize = TRUE)
y_hat_bic <- predict(model_bic, x)
ssr_bic <- t(y - y_hat_bic) %*% (y - y_hat_bic)
mse_bic = mean((y - y_hat_bic)^2)
mse_bic
x_scaled <- scale(x)
bic <- c()
lambdas_to_try <- 10^seq(-3, 0, length = 100)
for (lambda in seq(lambdas_to_try)) {
# Run model
model <- glmnet(x, y, alpha = 1, lambda = lambdas_to_try[lambda], standardize = TRUE)
# Extract coefficients and residuals (remove first row for the intercept)
betas <- as.vector((as.matrix(coef(model))[-1, ]))
resid <- y - (x_scaled %*% betas)
# Compute hat-matrix and degrees of freedom
ld <- lambdas_to_try[lambda] * diag(ncol(x_scaled))
H <- x_scaled %*% solve(t(x_scaled) %*% x_scaled + ld) %*% t(x_scaled)
#degrees of freedom is just the trace of the hat matrix
degfreedom <- tr(H)
# Compute information criteria
# assuming AIC(ridge) = nlog(e'e) + 2degfreedom(ridge)
#BIC(ridge) = nlog(e'e) + 2df(ridge)*log(n)
bic[lambda] <- nrow(x_scaled) * log(t(resid) %*% resid) + 2 * degfreedom * log(nrow(x_scaled))
}
# Optimal lambda
lambda_bic.lasso <- lambdas_to_try[which.min(bic)]
lambda_bic.lasso
setwd('/Users/kaijing/Documents/EC4304/Dow-Jones/Data')
df <- read.csv("final.csv")
#drop first two col of s/n and date respectively
#remove the inflation var that was causing issue
df <- subset(df, select=-c(X, Date, inf_pctchg))
rm(list = ls())
library(aTSA)
require(sqldf)
library(sqldf)
library(glmnet)
library(HDeconometrics)
##########################################################
#read data
#########################################################
setwd('/Users/kaijing/Documents/EC4304/Dow-Jones/Data')
df <- read.csv("final.csv")
#drop first two col of s/n and date respectively
#remove the inflation var that was causing issue
df <- subset(df, select=-c(X, Date, inf_pctchg))
#if there are non-finite datapoints, we just set it to NA
is.na(df) <- sapply(df, is.infinite)
#set all NA datapoints to 0
df[is.na(df)] <- 0
write.csv(df,'final.csv')
test = write.csv(df,'final.csv')
df <- read.csv("final.csv")
View(df)
without_na = df[complete.cases(df), ]
